from __future__ import annotations

import filecmp
import fnmatch
import re
import shutil
import socket
import subprocess
import tempfile
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

import yaml
from rich.console import Console

from .paths import get_sync_config_path, get_sync_repo_dir
from .system import check_command_exists, get_os, run_command

console = Console()


CLAUDE_IGNORE_PATTERNS = [
    "debug/",
    "cache/",
    "paste-cache/",
    "downloads/",
    "stats-cache.json",
    ".DS_Store",
    "Thumbs.db",
    "desktop.ini",
    "shell-snapshots/",
    "session-env/",
    "ide/",
    "statsig/",
    "telemetry/",
    "plugins/cache/",
    "plugins/marketplaces/",
    "plugins/repos/",
    "plugins/install-counts-cache.json",
    "plugins/installed_plugins.json",
    "plugins/known_marketplaces.json",
    ".credentials.json",
]

CLAUDE_MEM_IGNORE_PATTERNS = ["logs/", "worker.pid", "*.db-wal", "*.db-shm"]

GLOBAL_IGNORE_PATTERNS = [".DS_Store", "Thumbs.db", "desktop.ini"]

LFS_THRESHOLD_MB = 50
LFS_EXCLUDE_EXTENSIONS = {".jsonl"}


def default_sync_directories() -> list[dict[str, Any]]:
    return [
        {
            "path": "~/.claude",
            "repo_subdir": "claude",
            "ignore_profile": "claude",
            "custom_ignore": [],
        },
        {
            "path": "~/.claude-mem",
            "repo_subdir": "claude-mem",
            "ignore_profile": "claude-mem",
            "custom_ignore": [],
        },
    ]


def to_tilde_path(path: Path | str) -> str:
    resolved = Path(path).expanduser()
    home = Path.home()
    try:
        rel = resolved.relative_to(home)
        return f"~/{rel.as_posix()}"
    except ValueError:
        return str(resolved)


def get_ignore_patterns(
    profile: str, custom_ignore: list[str] | None = None
) -> list[str]:
    if profile == "claude":
        return list(CLAUDE_IGNORE_PATTERNS)
    if profile == "claude-mem":
        return list(CLAUDE_MEM_IGNORE_PATTERNS)
    if profile == "custom":
        return list(custom_ignore or [])

    console.print(
        f"[yellow]警告：未知 ignore profile '{profile}'，將使用空排除清單[/yellow]"
    )
    return []


def load_sync_config() -> dict[str, Any]:
    config_path = get_sync_config_path()
    if not config_path.exists():
        raise FileNotFoundError(
            f"找不到 sync 設定檔：{config_path}。請先執行 `ai-dev sync init`"
        )

    with open(config_path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f) or {}

    if not isinstance(data, dict):
        raise ValueError("sync.yaml 格式錯誤：必須為 YAML object")

    data.setdefault("version", "1")
    data.setdefault("remote", "")
    data.setdefault("last_sync", None)
    data.setdefault("directories", [])
    return data


def save_sync_config(config: dict[str, Any]) -> None:
    config_path = get_sync_config_path()
    config_path.parent.mkdir(parents=True, exist_ok=True)
    with open(config_path, "w", encoding="utf-8") as f:
        yaml.dump(
            config, f, allow_unicode=True, default_flow_style=False, sort_keys=False
        )


def prefix_excludes(repo_subdir: str, excludes: list[str]) -> list[str]:
    prefix = repo_subdir.strip("/")
    if not prefix:
        return []

    prefixed: list[str] = []
    for pattern in excludes:
        normalized = pattern.strip().lstrip("/")
        if not normalized:
            continue
        prefixed.append(f"{prefix}/{normalized}")
    return prefixed


def generate_gitignore(directories: list[dict[str, Any]]) -> str:
    lines = ["# Generated by ai-dev sync. Do not edit manually.", ""]
    seen: set[str] = set()

    for item in directories:
        repo_subdir = str(item.get("repo_subdir", "")).strip()
        if not repo_subdir:
            continue

        profile = str(item.get("ignore_profile", "custom"))
        custom_ignore = item.get("custom_ignore") or []
        excludes = get_ignore_patterns(profile, custom_ignore)
        prefixed = prefix_excludes(repo_subdir, excludes)
        if not prefixed:
            continue

        lines.append(f"# {repo_subdir}/ profile: {profile}")
        for rule in prefixed:
            if rule in seen:
                continue
            seen.add(rule)
            lines.append(rule)
        lines.append("")

    lines.append("# Global excludes")
    for rule in GLOBAL_IGNORE_PATTERNS:
        if rule in seen:
            continue
        seen.add(rule)
        lines.append(rule)

    return "\n".join(lines).rstrip() + "\n"


def generate_gitattributes(lfs_patterns: list[str] | None = None) -> str:
    lines = [
        "# Generated by ai-dev sync. Do not edit manually.",
        "",
        "*.jsonl merge=union",
        "*.md text eol=lf",
    ]

    if lfs_patterns:
        deduped_patterns = sorted(
            {pattern.strip() for pattern in lfs_patterns if pattern}
        )
        if deduped_patterns:
            lines.append("")
            lines.append("# Auto-detected Git LFS patterns")
            for pattern in deduped_patterns:
                lines.append(f"{pattern} filter=lfs diff=lfs merge=lfs -text")

    return "\n".join(lines).rstrip() + "\n"


def write_gitignore(repo_dir: Path | str, directories: list[dict[str, Any]]) -> Path:
    repo_root = Path(repo_dir).expanduser()
    repo_root.mkdir(parents=True, exist_ok=True)
    gitignore_path = repo_root / ".gitignore"
    gitignore_path.write_text(generate_gitignore(directories), encoding="utf-8")
    return gitignore_path


def write_gitattributes(
    repo_dir: Path | str, lfs_patterns: list[str] | None = None
) -> Path:
    repo_root = Path(repo_dir).expanduser()
    repo_root.mkdir(parents=True, exist_ok=True)
    gitattributes_path = repo_root / ".gitattributes"
    gitattributes_path.write_text(
        generate_gitattributes(lfs_patterns=lfs_patterns), encoding="utf-8"
    )
    return gitattributes_path


def check_lfs_available() -> bool:
    return shutil.which("git-lfs") is not None


def git_lfs_setup(repo_dir: Path | str) -> bool:
    repo_path = Path(repo_dir).expanduser()
    result = subprocess.run(
        ["git", "lfs", "install", "--local"],
        cwd=str(repo_path),
        capture_output=True,
        text=True,
        check=False,
    )
    return result.returncode == 0


def _is_hidden_path(rel_path: str) -> bool:
    return any(part.startswith(".") for part in Path(rel_path).parts)


def detect_lfs_patterns(
    repo_dir: Path | str, threshold_mb: int = LFS_THRESHOLD_MB
) -> list[str]:
    repo_root = Path(repo_dir).expanduser()
    if not repo_root.exists():
        return []

    threshold_bytes = threshold_mb * 1024 * 1024
    patterns: set[str] = set()

    for path in repo_root.rglob("*"):
        if not path.is_file():
            continue

        rel = path.relative_to(repo_root).as_posix()
        if _is_hidden_path(rel):
            continue

        try:
            size_bytes = path.stat().st_size
        except OSError:
            continue

        if size_bytes <= threshold_bytes:
            continue

        suffix = path.suffix.lower()
        if not suffix or suffix in LFS_EXCLUDE_EXTENSIONS:
            continue

        patterns.add(f"*{suffix}")

    return sorted(patterns)


def git_lfs_migrate_existing(repo_dir: Path | str, patterns: list[str]) -> bool:
    if not patterns:
        return True

    repo_path = Path(repo_dir).expanduser()
    has_head = subprocess.run(
        ["git", "rev-parse", "--verify", "HEAD"],
        cwd=str(repo_path),
        capture_output=True,
        text=True,
        check=False,
    )
    if has_head.returncode != 0:
        return True

    include = ",".join(patterns)
    result = subprocess.run(
        ["git", "lfs", "migrate", "import", f"--include={include}", "--everything"],
        cwd=str(repo_path),
        capture_output=True,
        text=True,
        check=False,
    )
    return result.returncode == 0


def _normalize_pattern(pattern: str) -> str:
    return pattern.strip().replace("\\", "/")


def _matches_pattern(rel_path: str, pattern: str) -> bool:
    normalized_path = rel_path.strip("/")
    normalized_pattern = _normalize_pattern(pattern).lstrip("/")
    if not normalized_pattern:
        return False

    if normalized_pattern.endswith("/"):
        base = normalized_pattern.rstrip("/")
        return normalized_path == base or normalized_path.startswith(f"{base}/")

    if fnmatch.fnmatch(normalized_path, normalized_pattern):
        return True

    return fnmatch.fnmatch(Path(normalized_path).name, normalized_pattern)


def _is_excluded(rel_path: str, excludes: list[str]) -> bool:
    return any(_matches_pattern(rel_path, pattern) for pattern in excludes)


def _parse_rsync_stat(output: str, field: str) -> int:
    pattern = rf"{re.escape(field)}:\s*([0-9]+)"
    match = re.search(pattern, output)
    if not match:
        return 0
    return int(match.group(1))


def _sync_directory_rsync(
    src: Path, dst: Path, excludes: list[str], delete: bool = True
) -> dict[str, Any]:
    with tempfile.NamedTemporaryFile(mode="w", encoding="utf-8", delete=False) as tmp:
        for pattern in excludes:
            tmp.write(pattern)
            tmp.write("\n")
        exclude_file = tmp.name

    try:
        command = [
            "rsync",
            "-av",
            "--stats",
            "--exclude-from",
            exclude_file,
        ]
        if delete:
            command.append("--delete")
        command.extend([f"{src}/", f"{dst}/"])

        result = subprocess.run(command, capture_output=True, text=True, check=False)
        if result.returncode != 0:
            message = result.stderr.strip() or "rsync 同步失敗"
            raise RuntimeError(message)

        output = result.stdout
        return {
            "added": 0,
            "updated": _parse_rsync_stat(output, "Number of regular files transferred"),
            "deleted": _parse_rsync_stat(output, "Number of deleted files"),
            "files": _parse_rsync_stat(output, "Number of files"),
            "method": "rsync",
        }
    finally:
        Path(exclude_file).unlink(missing_ok=True)


def _files_equal(left: Path, right: Path) -> bool:
    try:
        return filecmp.cmp(left, right, shallow=False)
    except OSError:
        return False


def _collect_files(root: Path, excludes: list[str]) -> dict[str, Path]:
    files: dict[str, Path] = {}
    if not root.exists():
        return files

    for path in root.rglob("*"):
        if not path.is_file():
            continue
        rel = path.relative_to(root).as_posix()
        if _is_excluded(rel, excludes):
            continue
        files[rel] = path
    return files


def _cleanup_empty_dirs(root: Path) -> None:
    if not root.exists():
        return
    for path in sorted(root.rglob("*"), key=lambda item: len(item.parts), reverse=True):
        if not path.is_dir():
            continue
        try:
            path.rmdir()
        except OSError:
            continue


def _sync_directory_shutil(
    src: Path, dst: Path, excludes: list[str], delete: bool = True
) -> dict[str, Any]:
    src_files = _collect_files(src, excludes)
    dst_files = _collect_files(dst, excludes)

    added = 0
    updated = 0
    deleted = 0

    for rel, src_file in src_files.items():
        dst_file = dst / rel
        dst_file.parent.mkdir(parents=True, exist_ok=True)

        if not dst_file.exists():
            shutil.copy2(src_file, dst_file)
            added += 1
            continue

        if not _files_equal(src_file, dst_file):
            shutil.copy2(src_file, dst_file)
            updated += 1

    if delete:
        for rel, dst_file in dst_files.items():
            if rel in src_files:
                continue
            dst_file.unlink(missing_ok=True)
            deleted += 1
        _cleanup_empty_dirs(dst)

    return {
        "added": added,
        "updated": updated,
        "deleted": deleted,
        "files": len(src_files),
        "method": "shutil",
    }


def sync_directory(
    src: Path | str,
    dst: Path | str,
    excludes: list[str],
    delete: bool = True,
) -> dict[str, Any]:
    src_path = Path(src).expanduser()
    dst_path = Path(dst).expanduser()

    if not src_path.exists():
        raise FileNotFoundError(f"來源目錄不存在：{src_path}")

    dst_path.mkdir(parents=True, exist_ok=True)

    if get_os() != "windows" and check_command_exists("rsync"):
        return _sync_directory_rsync(src_path, dst_path, excludes, delete=delete)

    return _sync_directory_shutil(src_path, dst_path, excludes, delete=delete)


def _git_has_remote(repo_dir: Path, remote_name: str) -> bool:
    result = subprocess.run(
        ["git", "remote", "get-url", remote_name],
        cwd=str(repo_dir),
        capture_output=True,
        text=True,
        check=False,
    )
    return result.returncode == 0


def _git_set_remote(repo_path: Path, remote_url: str) -> None:
    if _git_has_remote(repo_path, "origin"):
        run_command(
            ["git", "remote", "set-url", "origin", remote_url], cwd=str(repo_path)
        )
    else:
        run_command(["git", "remote", "add", "origin", remote_url], cwd=str(repo_path))


def _git_remote_has_branch(repo_path: Path, branch: str) -> bool:
    result = subprocess.run(
        ["git", "ls-remote", "--heads", "origin", branch],
        cwd=str(repo_path),
        capture_output=True,
        text=True,
        check=False,
    )
    return result.returncode == 0 and branch in result.stdout


def git_init_or_clone(repo_dir: Path | str, remote_url: str) -> str:
    """初始化或 clone sync repo。回傳 'existing' | 'cloned' | 'initialized'。"""
    repo_path = Path(repo_dir).expanduser()

    # 已有 .git → 更新 remote URL 並對齊遠端
    if (repo_path / ".git").exists():
        _git_set_remote(repo_path, remote_url)
        # fetch 遠端並對齊，避免 stale 狀態導致 push 失敗
        subprocess.run(
            ["git", "fetch", "origin"],
            cwd=str(repo_path),
            capture_output=True,
            text=True,
            check=False,
        )
        if _git_remote_has_branch(repo_path, "main"):
            # 設定 upstream tracking
            subprocess.run(
                ["git", "branch", "--set-upstream-to=origin/main", "main"],
                cwd=str(repo_path),
                capture_output=True,
                check=False,
            )
            # rebase 本地變更到遠端之上
            subprocess.run(
                ["git", "rebase", "origin/main"],
                cwd=str(repo_path),
                capture_output=True,
                check=False,
            )
        return "existing"

    repo_path.parent.mkdir(parents=True, exist_ok=True)

    # 嘗試 clone
    if not repo_path.exists():
        clone_result = subprocess.run(
            ["git", "clone", remote_url, str(repo_path)],
            capture_output=True,
            text=True,
            check=False,
        )
        if clone_result.returncode == 0:
            return "cloned"
        # clone 失敗，清理殘留目錄
        if repo_path.exists():
            shutil.rmtree(repo_path)

    # fallback: init + fetch
    repo_path.mkdir(parents=True, exist_ok=True)
    run_command(["git", "init"], cwd=str(repo_path))
    _git_set_remote(repo_path, remote_url)

    # 嘗試 fetch 遠端內容
    fetch_result = subprocess.run(
        ["git", "fetch", "origin"],
        cwd=str(repo_path),
        capture_output=True,
        text=True,
        check=False,
    )

    if fetch_result.returncode == 0 and _git_remote_has_branch(repo_path, "main"):
        # 遠端有 main → 以遠端為基礎
        run_command(
            ["git", "checkout", "-b", "main", "--track", "origin/main"],
            cwd=str(repo_path),
            check=False,
        )
        return "cloned"

    # 遠端為空或 fetch 失敗 → 本地全新開始
    run_command(["git", "branch", "-M", "main"], cwd=str(repo_path), check=False)
    return "initialized"


def git_add_commit(repo_dir: Path | str, message: str) -> bool:
    repo_path = Path(repo_dir).expanduser()
    run_command(["git", "add", "-A"], cwd=str(repo_path))

    has_changes = subprocess.run(
        ["git", "diff", "--cached", "--quiet"],
        cwd=str(repo_path),
        check=False,
    )
    if has_changes.returncode == 0:
        return False

    result = run_command(
        ["git", "commit", "-m", message],
        cwd=str(repo_path),
        check=False,
    )
    return result.returncode == 0


def git_pull_rebase(repo_dir: Path | str) -> bool:
    repo_path = Path(repo_dir).expanduser()
    if not _has_upstream(repo_path):
        return True  # 尚未設定 upstream，跳過 pull
    result = run_command(["git", "pull", "--rebase"], cwd=str(repo_path), check=False)
    return result.returncode == 0


def _has_upstream(repo_dir: Path) -> bool:
    result = subprocess.run(
        ["git", "rev-parse", "--abbrev-ref", "--symbolic-full-name", "@{upstream}"],
        cwd=str(repo_dir),
        capture_output=True,
        text=True,
        check=False,
    )
    return result.returncode == 0


def git_push(repo_dir: Path | str) -> bool:
    repo_path = Path(repo_dir).expanduser()
    if _has_upstream(repo_path):
        cmd = ["git", "push"]
    else:
        cmd = ["git", "push", "-u", "origin", "main"]
    result = run_command(cmd, cwd=str(repo_path), check=False)
    return result.returncode == 0


def git_status_summary(repo_dir: Path | str) -> dict[str, int]:
    repo_path = Path(repo_dir).expanduser()

    local_changes_result = subprocess.run(
        ["git", "status", "--porcelain"],
        cwd=str(repo_path),
        capture_output=True,
        text=True,
        check=False,
    )
    if local_changes_result.returncode == 0:
        local_changes = len(
            [line for line in local_changes_result.stdout.splitlines() if line.strip()]
        )
    else:
        local_changes = 0

    if _has_upstream(repo_path):
        behind_result = subprocess.run(
            ["git", "rev-list", "--count", "HEAD..@{upstream}"],
            cwd=str(repo_path),
            capture_output=True,
            text=True,
            check=False,
        )
        if behind_result.returncode == 0:
            try:
                behind_count = int(behind_result.stdout.strip() or "0")
            except ValueError:
                behind_count = 0
        else:
            behind_count = 0
    else:
        behind_count = 0

    return {"local_changes": local_changes, "behind_count": behind_count}


def get_hostname() -> str:
    return socket.gethostname()


def get_timestamp() -> str:
    return datetime.now().strftime("%Y-%m-%d-%H%M")


def generate_sync_commit_message() -> str:
    return f"sync: {get_hostname()} {get_timestamp()}"


def now_iso8601() -> str:
    return datetime.now(timezone.utc).isoformat()


def count_directory_changes(
    source_dir: Path | str,
    target_dir: Path | str,
    excludes: list[str],
) -> int:
    source_files = _collect_files(Path(source_dir).expanduser(), excludes)
    target_files = _collect_files(Path(target_dir).expanduser(), excludes)

    source_set = set(source_files.keys())
    target_set = set(target_files.keys())

    added_or_removed = len(source_set.symmetric_difference(target_set))
    modified = 0
    for rel in source_set.intersection(target_set):
        if not _files_equal(source_files[rel], target_files[rel]):
            modified += 1

    return added_or_removed + modified


def _prefixed_change_path(repo_subdir: str, rel_path: str) -> str:
    normalized_subdir = repo_subdir.strip().strip("/")
    normalized_rel = rel_path.strip().strip("/")
    if not normalized_subdir:
        return normalized_rel
    if not normalized_rel:
        return normalized_subdir
    return f"{normalized_subdir}/{normalized_rel}"


def detect_local_changes(config: dict[str, Any]) -> dict[str, Any]:
    repo_dir = get_sync_repo_dir()
    result: dict[str, Any] = {
        "total_changes": 0,
        "directories": [],
        "files": [],
    }

    for item in config.get("directories", []):
        local_path = Path(str(item.get("path", ""))).expanduser()
        repo_subdir = str(item.get("repo_subdir", "")).strip()
        repo_path = repo_dir / repo_subdir if repo_subdir else repo_dir
        excludes = get_ignore_patterns(
            str(item.get("ignore_profile", "custom")), item.get("custom_ignore", [])
        )

        local_files = _collect_files(local_path, excludes)
        repo_files = _collect_files(repo_path, excludes)

        local_set = set(local_files.keys())
        repo_set = set(repo_files.keys())

        added = sorted(local_set - repo_set)
        deleted = sorted(repo_set - local_set)
        modified = sorted(
            rel
            for rel in local_set.intersection(repo_set)
            if not _files_equal(local_files[rel], repo_files[rel])
        )

        total = len(added) + len(modified) + len(deleted)
        if total == 0:
            continue

        prefixed_added = [_prefixed_change_path(repo_subdir, rel) for rel in added]
        prefixed_modified = [
            _prefixed_change_path(repo_subdir, rel) for rel in modified
        ]
        prefixed_deleted = [_prefixed_change_path(repo_subdir, rel) for rel in deleted]

        result["directories"].append(
            {
                "path": str(item.get("path", "")),
                "repo_subdir": repo_subdir,
                "added": prefixed_added,
                "modified": prefixed_modified,
                "deleted": prefixed_deleted,
                "total_changes": total,
            }
        )

        for rel_path in prefixed_added:
            result["files"].append({"path": rel_path, "type": "added"})
        for rel_path in prefixed_modified:
            result["files"].append({"path": rel_path, "type": "modified"})
        for rel_path in prefixed_deleted:
            result["files"].append({"path": rel_path, "type": "deleted"})

        result["total_changes"] += total

    result["files"] = sorted(
        result["files"],
        key=lambda item: (str(item.get("path", "")), str(item.get("type", ""))),
    )

    return result


def make_repo_subdir_name(path: Path | str, existing: set[str]) -> str:
    target = Path(path).expanduser()
    raw_name = target.name or "sync-dir"
    base = re.sub(r"[^A-Za-z0-9._-]+", "-", raw_name).strip("-").lower() or "sync-dir"

    candidate = base
    index = 2
    while candidate in existing:
        candidate = f"{base}-{index}"
        index += 1
    return candidate


def ensure_sync_repo_dir() -> Path:
    repo_dir = get_sync_repo_dir()
    repo_dir.mkdir(parents=True, exist_ok=True)
    return repo_dir


# ---------------------------------------------------------------------------
# Plugin manifest: 可攜帶的 plugin 狀態記錄
# ---------------------------------------------------------------------------

PLUGIN_MANIFEST_NAME = "plugin-manifest.json"


def generate_plugin_manifest(claude_dir: Path | str) -> dict[str, Any] | None:
    """從 ~/.claude 讀取 plugin metadata，產生可攜帶 manifest（無絕對路徑）。"""
    import json

    claude_path = Path(claude_dir).expanduser()
    installed_path = claude_path / "plugins" / "installed_plugins.json"
    marketplaces_path = claude_path / "plugins" / "known_marketplaces.json"
    settings_path = claude_path / "settings.json"

    if not installed_path.exists() and not marketplaces_path.exists():
        return None

    manifest: dict[str, Any] = {"version": 1, "marketplaces": {}, "plugins": []}

    # 提取 marketplace 來源（只保留 source，去除本機路徑）
    if marketplaces_path.exists():
        try:
            with open(marketplaces_path, "r", encoding="utf-8") as f:
                raw = json.load(f)
            for name, info in raw.items():
                source = info.get("source", {})
                manifest["marketplaces"][name] = source
        except (json.JSONDecodeError, OSError):
            pass

    # 提取已安裝 plugin（只保留名稱、版本、marketplace）
    if installed_path.exists():
        try:
            with open(installed_path, "r", encoding="utf-8") as f:
                raw = json.load(f)
            for plugin_key, entries in raw.get("plugins", {}).items():
                if not entries:
                    continue
                latest = entries[-1]
                manifest["plugins"].append(
                    {
                        "name": plugin_key,
                        "version": latest.get("version", ""),
                        "scope": latest.get("scope", "user"),
                    }
                )
        except (json.JSONDecodeError, OSError):
            pass

    # 從 settings.json 補充 enabledPlugins
    if settings_path.exists():
        try:
            with open(settings_path, "r", encoding="utf-8") as f:
                settings = json.load(f)
            manifest["enabledPlugins"] = list(settings.get("enabledPlugins", {}).keys())
        except (json.JSONDecodeError, OSError):
            pass

    return manifest


def save_plugin_manifest(
    repo_dir: Path | str, claude_subdir: str, claude_dir: Path | str | None = None
) -> bool:
    """在 push 時產生 plugin manifest 並存入 sync repo。"""
    import json

    repo_path = Path(repo_dir).expanduser()

    if claude_dir is None:
        claude_dir = _find_claude_dir_for_subdir(claude_subdir)
    if not claude_dir:
        return False

    manifest = generate_plugin_manifest(claude_dir)
    if not manifest:
        return False

    manifest_path = repo_path / claude_subdir / "plugins" / PLUGIN_MANIFEST_NAME
    manifest_path.parent.mkdir(parents=True, exist_ok=True)
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(manifest, f, indent=2, ensure_ascii=False)
    return True


def restore_plugins_on_pull(repo_dir: Path | str, claude_subdir: str) -> dict[str, Any]:
    """在 init/pull 時讀取 manifest，自動安裝 marketplace 與 plugin。

    回傳 {"installed": [...], "skipped": [...], "manifest": dict}
    """
    import json

    repo_path = Path(repo_dir).expanduser()
    manifest_path = repo_path / claude_subdir / "plugins" / PLUGIN_MANIFEST_NAME
    result: dict[str, Any] = {"installed": [], "skipped": [], "manifest": None}

    if not manifest_path.exists():
        return result

    try:
        with open(manifest_path, "r", encoding="utf-8") as f:
            manifest = json.load(f)
    except (json.JSONDecodeError, OSError):
        return result

    result["manifest"] = manifest
    claude_dir = Path("~/.claude").expanduser()
    plugins_dir = claude_dir / "plugins"
    plugins_dir.mkdir(parents=True, exist_ok=True)

    # 1. Clone marketplace repos
    marketplaces = manifest.get("marketplaces", {})
    cloned_marketplaces: set[str] = set()

    for name, source in marketplaces.items():
        marketplace_dir = plugins_dir / "marketplaces" / name
        if marketplace_dir.exists():
            cloned_marketplaces.add(name)
            continue

        clone_url = _marketplace_clone_url(source)
        if not clone_url:
            console.print(
                f"[yellow]  跳過 marketplace {name}："
                f"source 類型 '{source.get('source')}' 無法自動 clone[/yellow]"
            )
            continue

        marketplace_dir.parent.mkdir(parents=True, exist_ok=True)
        clone_result = subprocess.run(
            ["git", "clone", "--depth", "1", clone_url, str(marketplace_dir)],
            capture_output=True,
            text=True,
            check=False,
        )
        if clone_result.returncode == 0:
            cloned_marketplaces.add(name)
        else:
            console.print(
                f"[yellow]  clone marketplace {name} 失敗：{clone_result.stderr.strip()[:100]}[/yellow]"
            )

    # 2. 寫入 known_marketplaces.json（本機路徑）
    known: dict[str, Any] = {}
    for name, source in marketplaces.items():
        if name not in cloned_marketplaces:
            continue
        marketplace_dir = plugins_dir / "marketplaces" / name
        known[name] = {
            "source": source,
            "installLocation": str(marketplace_dir),
            "lastUpdated": now_iso8601(),
        }
    if known:
        with open(plugins_dir / "known_marketplaces.json", "w", encoding="utf-8") as f:
            json.dump(known, f, indent=2, ensure_ascii=False)

    # 2.5 讀取各 marketplace 的 marketplace.json（用於解析 plugin 來源路徑）
    marketplace_plugin_map: dict[str, dict[str, Any]] = {}
    for name in cloned_marketplaces:
        mkt_json = (
            plugins_dir / "marketplaces" / name / ".claude-plugin" / "marketplace.json"
        )
        if not mkt_json.exists():
            continue
        try:
            with open(mkt_json, "r", encoding="utf-8") as f:
                mkt_data = json.load(f)
            for p in mkt_data.get("plugins", []):
                p_name = p.get("name", "")
                if p_name:
                    marketplace_plugin_map[f"{p_name}@{name}"] = p
        except (json.JSONDecodeError, OSError):
            pass

    # 3. 安裝 plugin：根據 marketplace.json 的 source 解析來源
    installed_plugins: dict[str, list[dict[str, Any]]] = {}
    enabled_plugins: dict[str, bool] = {}

    for plugin_info in manifest.get("plugins", []):
        plugin_key = plugin_info[
            "name"
        ]  # e.g. "code-simplifier@claude-plugins-official"
        version = plugin_info.get("version", "latest")
        scope = plugin_info.get("scope", "user")

        parts = plugin_key.split("@", 1)
        if len(parts) != 2:
            result["skipped"].append(plugin_key)
            continue
        plugin_name, marketplace_name = parts

        if marketplace_name not in cloned_marketplaces:
            result["skipped"].append(plugin_key)
            continue

        marketplace_dir = plugins_dir / "marketplaces" / marketplace_name
        plugin_src = _resolve_plugin_source(
            plugin_key,
            plugin_name,
            marketplace_dir,
            marketplace_plugin_map,
            plugins_dir,
        )
        if not plugin_src:
            result["skipped"].append(plugin_key)
            continue

        # 複製到 cache
        cache_dir = plugins_dir / "cache" / marketplace_name / plugin_name / version
        if cache_dir.exists():
            shutil.rmtree(cache_dir)
        cache_dir.parent.mkdir(parents=True, exist_ok=True)
        shutil.copytree(plugin_src, cache_dir)

        installed_plugins[plugin_key] = [
            {
                "scope": scope,
                "installPath": str(cache_dir),
                "version": version,
                "installedAt": now_iso8601(),
                "lastUpdated": now_iso8601(),
            }
        ]
        enabled_plugins[plugin_key] = True
        result["installed"].append(plugin_key)

    # 4. 寫入 installed_plugins.json
    if installed_plugins:
        with open(plugins_dir / "installed_plugins.json", "w", encoding="utf-8") as f:
            json.dump(
                {"version": 2, "plugins": installed_plugins},
                f,
                indent=2,
                ensure_ascii=False,
            )

    # 5. 還原 settings.json 的 enabledPlugins（已安裝的才啟用）
    settings_path = claude_dir / "settings.json"
    if settings_path.exists() and enabled_plugins:
        try:
            with open(settings_path, "r", encoding="utf-8") as f:
                settings = json.load(f)
            settings["enabledPlugins"] = enabled_plugins
            with open(settings_path, "w", encoding="utf-8") as f:
                json.dump(settings, f, indent=2, ensure_ascii=False)
        except (json.JSONDecodeError, OSError):
            pass

    return result


def _resolve_plugin_source(
    plugin_key: str,
    plugin_name: str,
    marketplace_dir: Path,
    marketplace_plugin_map: dict[str, Any],
    plugins_dir: Path,
) -> Path | None:
    """根據 marketplace.json 的 source 欄位解析 plugin 來源目錄。

    支援三種 source 型別：
    - 相對路徑字串 (e.g. "./plugins/foo", "./plugin") → marketplace 內的子目錄
    - URL 物件 (e.g. {"source": "url", "url": "https://...git"}) → 另外 clone
    - 不在 marketplace.json 中 → fallback 到 plugins/<name>
    """
    mkt_entry = marketplace_plugin_map.get(plugin_key)

    if mkt_entry:
        source = mkt_entry.get("source")

        # 相對路徑字串：marketplace 內的子目錄
        if isinstance(source, str):
            resolved = (marketplace_dir / source).resolve()
            if resolved.exists():
                return resolved

        # URL 物件：從外部 repo clone
        elif isinstance(source, dict):
            clone_url = source.get("url")
            if clone_url:
                # clone 到 repos/<marketplace>/<plugin>
                repo_dest = plugins_dir / "repos" / plugin_key.replace("@", "/")
                if not repo_dest.exists():
                    repo_dest.parent.mkdir(parents=True, exist_ok=True)
                    clone_result = subprocess.run(
                        ["git", "clone", "--depth", "1", clone_url, str(repo_dest)],
                        capture_output=True,
                        text=True,
                        check=False,
                    )
                    if clone_result.returncode != 0:
                        console.print(
                            f"[yellow]  clone plugin {plugin_key} 失敗："
                            f"{clone_result.stderr.strip()[:100]}[/yellow]"
                        )
                        return None
                return repo_dest

    # fallback：標準 plugins/<name> 目錄
    fallback = marketplace_dir / "plugins" / plugin_name
    return fallback if fallback.exists() else None


def _marketplace_clone_url(source: dict[str, Any]) -> str | None:
    """從 marketplace source 產生可 clone 的 URL。"""
    src_type = source.get("source", "")
    if src_type == "github":
        repo = source.get("repo", "")
        return f"https://github.com/{repo}.git" if repo else None
    if src_type == "git":
        return source.get("url")
    # directory 類型無法遠端 clone
    return None


def _find_claude_dir_for_subdir(claude_subdir: str) -> Path | None:
    """從 sync config 找到 subdir 對應的本機路徑。"""
    try:
        config = load_sync_config()
    except FileNotFoundError:
        return None
    for item in config.get("directories", []):
        if item.get("repo_subdir") == claude_subdir:
            return Path(item["path"]).expanduser()
    return None


def get_claude_subdir(config: dict[str, Any]) -> str | None:
    """找到 ignore_profile 為 'claude' 的 repo_subdir。"""
    for item in config.get("directories", []):
        if item.get("ignore_profile") == "claude":
            return str(item.get("repo_subdir", ""))
    return None
